/**
 * Copyright (c) Microsoft Corporation
 * <p/>
 * All rights reserved.
 * <p/>
 * MIT License
 * <p/>
 * Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
 * documentation files (the "Software"), to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
 * to permit persons to whom the Software is furnished to do so, subject to the following conditions:
 * <p/>
 * The above copyright notice and this permission notice shall be included in all copies or substantial portions of
 * the Software.
 * <p/>
 * THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
 * THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */
package com.microsoft.azure.maven.spark.restapi;

import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.Nullable;

import java.util.*;
import java.util.regex.Pattern;

@JsonInclude(JsonInclude.Include.NON_EMPTY)
public class SparkSubmissionParameter implements Convertible {
    /**
     * For interactive spark job:
     * <p>
     * kind	            The session kind (required)	session kind
     * proxyUser	        The user to impersonate that will run this session (e.g. bob)	                string
     * jars	            Files to be placed on the java classpath	                                    list of paths
     * pyFiles	            Files to be placed on the PYTHONPATH	                                        list of paths
     * files	            Files to be placed in executor working directory	                            list of paths
     * driverMemory	    Memory for driver (e.g. 1000M, 2G)	                                            string
     * driverCores	        Number of cores used by driver (YARN mode only)	                                int
     * executorMemory	    Memory for executor (e.g. 1000M, 2G)	                                        string
     * executorCores	    Number of cores used by executor	                                            int
     * numExecutors	    Number of executors (YARN mode only)	                                        int
     * archives	        Archives to be uncompressed in the executor working directory (YARN mode only)	list of paths
     * queue	            The YARN queue to submit too (YARN mode only)	string
     * name	            Name of the application	string
     * conf             Spark configuration properties  Map of key=val
     */
    private String name = "";
    private String file = "";
    private String className = "";

    private String clusterName = "";
    private boolean isLocalArtifact = false;
    private String artifactName = "";
    private String localArtifactPath = "";
    private List<String> files = new ArrayList<>();
    private List<String> jars = new ArrayList<>();
    private List<String> args = new ArrayList<>();
    private Map<String, Object> jobConfig = new HashMap<>();

    private static final Pattern memorySizeRegex = Pattern.compile(	"\\d+(.\\d+)?[gGmM]");

    public static final String DriverMemory = "driverMemory";
    public static final String DriverMemoryDefaultValue = "4G";

    public static final String DriverCores = "driverCores";
    public static final int DriverCoresDefaultValue = 1;

    public static final String ExecutorMemory = "executorMemory";
    public static final String ExecutorMemoryDefaultValue = "4G";

    public static final String NumExecutors = "numExecutors";
    public static final int NumExecutorsDefaultValue = 5;

    public static final String ExecutorCores = "executorCores";
    public static final int ExecutorCoresDefaultValue = 1;

    public static final String Conf = "conf";   // 	Spark configuration properties

    public static final String NAME = "name";

    public SparkSubmissionParameter() {
    }

    public SparkSubmissionParameter(String clusterName,
                                    boolean isLocalArtifact,
                                    String artifactName,
                                    String localArtifactPath,
                                    String filePath,
                                    String className,
                                    List<String> referencedFiles,
                                    List<String> referencedJars,
                                    List<String> args,
                                    Map<String, Object> jobConfig) {
        this.clusterName = clusterName;
        this.isLocalArtifact = isLocalArtifact;
        this.artifactName = artifactName;
        this.localArtifactPath = localArtifactPath;
        this.file = filePath;
        this.className = className;
        this.files = referencedFiles;
        this.jars = referencedJars;
        this.jobConfig = jobConfig;
        this.args = args;
    }

    public void setName(@NonNull String name) {
        this.name = name;
    }

    public void setFile(String file) {
        this.file = file;
    }

    public void setClassName(String className) {
        this.className = className;
    }

    public void setClusterName(String clusterName) {
        this.clusterName = clusterName;
    }

    public void setLocalArtifact(boolean localArtifact) {
        isLocalArtifact = localArtifact;
    }

    public void setArtifactName(String artifactName) {
        this.artifactName = artifactName;
    }

    @JsonIgnore
    public String getClusterName() {
        return clusterName;
    }

    @JsonIgnore
    public boolean isLocalArtifact() {
        return isLocalArtifact;
    }

    @JsonIgnore
    public String getArtifactName() {
        return artifactName;
    }

    @JsonIgnore
    @Nullable
    public String getLocalArtifactPath() {
        return localArtifactPath;
    }

    public void setLocalArtifactPath(String path) {
        localArtifactPath = path;
    }

    @JsonProperty(NAME)
    public String getName() {
        return name;
    }

    @JsonProperty("file")
    @Nullable
    public String getFile() {
        return file;
    }

    @JsonProperty("className")
    public String getMainClassName() {
        return className;
    }

    @JsonProperty("files")
    @Nullable
    public List<String> getReferencedFiles() {
        return files;
    }

    @JsonProperty("jars")
    @Nullable
    public List<String> getReferencedJars() {
        return jars;
    }

    @JsonProperty("args")
    @Nullable
    public List<String> getArgs() {
        return args;
    }

    @JsonIgnore
    @Nullable
    public Map<String, Object> getJobConfig() {
        return jobConfig;
    }

    public void setFilePath(String filePath) {
        this.file = filePath;
    }

    @JsonProperty("driverMemory")
    @Nullable
    public String getDriverMemory() {
        return (String) jobConfig.get(DriverMemory);
    }

    @JsonProperty("driverCores")
    @Nullable
    public Integer getDriverCores() {
        return parseIntegerSafety(jobConfig.get(DriverCores));
    }

    @JsonProperty("executorMemory")
    @Nullable
    public String getExecutorMemory() {
        return (String) jobConfig.get(ExecutorMemory);
    }

    @JsonProperty("executorCores")
    @Nullable
    public Integer getExecutorCores() {
        return parseIntegerSafety(jobConfig.get(ExecutorCores));
    }

    @JsonProperty("numExecutors")
    @Nullable
    public Integer getNumExecutors() {
        return parseIntegerSafety(jobConfig.get(NumExecutors));
    }

    @JsonProperty("conf")
    @Nullable
    public Map<String, String> getConf() {
        Map<String, String> jobConf = new HashMap<>();

        Optional.ofNullable(jobConfig.get(Conf))
                .filter(Map.class::isInstance)
                .map(Map.class::cast)
                .ifPresent(conf -> conf.forEach((k, v) -> jobConf.put((String) k, (String) v)));

        return jobConf.isEmpty() ? null : jobConf;
    }

    @JsonIgnore
    @Nullable
    private Integer parseIntegerSafety(@Nullable Object maybeInteger) {
        if (maybeInteger == null) {
            return null;
        }

        if (maybeInteger instanceof Integer) {
            return (Integer) maybeInteger;
        }

        try {
            return Integer.parseInt(maybeInteger.toString());
        } catch (Exception ignored) {
            return null;
        }
    }

//    public static List<SparkSubmissionJobConfigCheckResult> checkJobConfigMap(Map<String, String> jobConfigMap) {
//
//        List<SparkSubmissionJobConfigCheckResult> messageList = new ArrayList<>();
//        for (Map.Entry<String, String> entry : jobConfigMap.entrySet()) {
//            String entryKey = entry.getKey();
//            if (StringUtils.isBlank(entryKey)) {
//                continue;
//            }
//
//            if (StringUtils.containsWhitespace(entryKey)) {
//                messageList.add(new SparkSubmissionJobConfigCheckResult(SparkSubmissionJobConfigCheckStatus.Error,
//                        "Error : The Spark config key with whitespace is not allowed"));
//
//                continue;
//            }
//
//            if (StringUtils.isBlank(entry.getValue())) {
//                messageList.add(new SparkSubmissionJobConfigCheckResult(SparkSubmissionJobConfigCheckStatus.Warning,
//                        "Warning : Empty value(s) will be override by default value(s) of system"));
//
//                continue;
//            }
//
//            if (entryKey.equals(DriverCores)
//                    || entryKey.equals(NumExecutors)
//                    || entryKey.equals(ExecutorCores)) {
//                try {
//                    Integer.parseInt(entry.getValue());
//                } catch (NumberFormatException e) {
//                    messageList.add(new SparkSubmissionJobConfigCheckResult(SparkSubmissionJobConfigCheckStatus.Error,
//                            String.format("Error : Key \"%s\" failed to parse the value \"%s\", it should be an integer", entry.getKey(), entry.getValue())));
//                }
//            } else if (entryKey.equals(DriverMemory)
//                    || entryKey.equals(ExecutorMemory)) {
//                if (!memorySizeRegex.matcher(entry.getValue()).matches()) {
//                    messageList.add(new SparkSubmissionJobConfigCheckResult(SparkSubmissionJobConfigCheckStatus.Error,
//                            String.format("Error : Key \"%s\" failed to parse the value \"%s\" into the memory size, it should be like 1.5G, 500M or 4g", entry.getKey(), entry.getValue())));
//                }
//            }
//        }
//
//        Collections.sort(messageList);
//
//        return messageList;
//    }

//    @NonNull
//    public List<Pair<String, String>> flatJobConfig() {
//        List<Pair<String, String>> flattedConfigs = new ArrayList<>();
//
//        getJobConfig().forEach((key, value) -> {
//            if (isSubmissionParameter(key)) {
//                flattedConfigs.add(new Pair<>(key, value == null ? null : value.toString()));
//            } else if (key.equals(Conf)) {
//                new SparkConfigures(value).forEach((scKey, scValue) ->
//                        flattedConfigs.add(new Pair<>(scKey, scValue == null ? null : scValue.toString())));
//            }
//        });
//
//        return flattedConfigs;
//    }
//
//    public void applyFlattedJobConf(List<Pair<String, String>> jobConfFlatted) {
//        jobConfig.clear();
//
//        SparkConfigures sparkConfig = new SparkConfigures();
//
//        jobConfFlatted.forEach(kvPair -> {
//            if (isSubmissionParameter(kvPair.first())) {
//                jobConfig.put(kvPair.first(), kvPair.second());
//            } else {
//                sparkConfig.put(kvPair.first(), kvPair.second());
//            }
//        });
//
//        if (!sparkConfig.isEmpty()) {
//            jobConfig.put(Conf, sparkConfig);
//        }
//    }

    public String serializeToJson() {
        return convertToJson().orElse("");
    }

    public static final String[] parameterList = new String[]{SparkSubmissionParameter.DriverMemory, SparkSubmissionParameter.DriverCores,
            SparkSubmissionParameter.ExecutorMemory, SparkSubmissionParameter.ExecutorCores, SparkSubmissionParameter.NumExecutors};

    //the first value in pair should be in same order with parameterList
//    public static final Pair<String, Object>[] defaultParameters = new Pair[]{
//            new Pair<>(SparkSubmissionParameter.DriverMemory, SparkSubmissionParameter.DriverMemoryDefaultValue),
//            new Pair<>(SparkSubmissionParameter.DriverCores, SparkSubmissionParameter.DriverCoresDefaultValue),
//            new Pair<>(SparkSubmissionParameter.ExecutorMemory, SparkSubmissionParameter.ExecutorMemoryDefaultValue),
//            new Pair<>(SparkSubmissionParameter.ExecutorCores, SparkSubmissionParameter.ExecutorCoresDefaultValue),
//            new Pair<>(SparkSubmissionParameter.NumExecutors, SparkSubmissionParameter.NumExecutorsDefaultValue)
//    };

    /**
     * Checks whether the key is one of Spark Job submission parameters or not
     *
     * @param key the key string to check
     * @return true if the key is a member of submission parameters; false otherwise
     */
    public static boolean isSubmissionParameter(String key) {
        return Arrays.stream(SparkSubmissionParameter.parameterList).anyMatch(key::equals);
    }
}